{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.parse import quote # 아스키 코드 형식이 아닌 글자를 url 인코딩 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 시작날짜, 끝날짜 받기)\n",
    "def MakeUrl(start_date, end_date):\n",
    "    urls= [] # 링크를 넣을 리스트 생성\n",
    "    # 검색 키워드\n",
    "    k = ['치과', '치과비용', '충치', '치아교정', '잇몸치료', '치주염', '구내염', '치아미백', '시린이', '잇몸영양제']\n",
    "    for keyword in k: \n",
    "        url = 'https://search.naver.com/search.naver?where=blog&query=' + quote(keyword) + f'&sm=tab_opt&nso=so:r,p:from{start_date}to{end_date}' # 시작날짜, 끝날짜\n",
    "        urls.append(url)\n",
    "    return urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목, 링크, 발행날짜가 담긴 블로그 상위 10위까지 크롤링\n",
    "def make_crawling(urls, start_date, end_date):\n",
    "    # 로봇 아닙니다 인식하기 위해 임시로 넣는 코드\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "    # 엑셀 파일 생성\n",
    "    writer=pd.ExcelWriter(f'C:\\\\Users\\\\User\\\\Desktop\\\\fproject\\\\매일블로그크롤링\\\\{start_date}~{end_date}.xlsx', engine='openpyxl')\n",
    "    k = ['치과', '치과비용', '충치', '치아교정', '잇몸치료', '치주염', '구내염', '치아미백', '시린이', '잇몸영양제']\n",
    "    \n",
    "    # 블로그 제목, 링크, 발행날짜 뽑기\n",
    "    idx = 0 # 키워드별로 엑셀 시트를 만들기 위해 인덱스 설정\n",
    "    for url in urls: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')  \n",
    "\n",
    "        title_list = []\n",
    "        link_list = []\n",
    "        pubdate_list = []\n",
    "\n",
    "        for j in range(1, 11):\n",
    "            items= soup.find_all('li', {'id':f'sp_blog_{j}'}) \n",
    "            # find_all로 가져오면 리스트형태로 가져옴\n",
    "            # print(item)\n",
    "\n",
    "            for item in items:\n",
    "                print('\\n') # 문자열 줄바꿈\n",
    "                # 블로그 게시글 제목\n",
    "                title = item.find('a',{'class':'api_txt_lines total_tit'}).get_text()\n",
    "                title_list.append(title)\n",
    "                print('제목 : ', title)\n",
    "                \n",
    "                # 링크 추출\n",
    "                link = item.find('a',{'class':'api_txt_lines total_tit'})['href']\n",
    "                link_list.append(link)\n",
    "                print('링크 : ', link)    \n",
    "                \n",
    "                # 발행날짜 뽑기\n",
    "                pubdate = item.find('span',{'class':'sub_time sub_txt'}).text.strip()\n",
    "                pubdate = pubdate.rstrip('.') # 오른쪽 끝단의 '.' 삭제\n",
    "                pubdate = pubdate.replace('.', '-')\n",
    "\n",
    "                # 3시간 전, 4일 전, 어제 따로 처리\n",
    "                now = datetime.datetime.now() # 오늘 날짜 불러오기\n",
    "                if '시간 전' in pubdate:\n",
    "                    pubdate = now.strftime('%Y-%m-%d')\n",
    "                    pubdate_list.append(pubdate)\n",
    "                elif '일 전' in pubdate:\n",
    "                    temp = int(pubdate[0:1])\n",
    "                    temp = datetime.timedelta(days=temp)\n",
    "                    pubdate = datetime.datetime.now() - temp\n",
    "                    pubdate = pubdate.strftime('%Y-%m-%d')\n",
    "                    pubdate_list.append(pubdate)\n",
    "                elif '어제' in pubdate:\n",
    "                    temp = datetime.timedelta(days=1)\n",
    "                    pubdate = datetime.datetime.now() - temp\n",
    "                    pubdate = pubdate.strftime('%Y-%m-%d')\n",
    "                    pubdate_list.append(pubdate)\n",
    "                else:\n",
    "                    pubdate_list.append(pubdate)\n",
    "                print('발행일자 : ',pubdate)\n",
    "\n",
    "    # 제목, 링크, 발행일자를 Dataframe으로 만들기 \n",
    "        df = pd.DataFrame({'제목':title_list, '링크':link_list, '발행일자':pubdate_list})\n",
    "    # 키워드당 하나의 시트로 만들어서 엑셀 파일 하나에 저장 \n",
    "        df.to_excel(writer, sheet_name=f'{k[idx]}')\n",
    "        idx += 1\n",
    "        writer.save()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수 전체 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 함수 : 함수 한꺼번에 실행\n",
    "if __name__=='__main__':\n",
    "    # 날짜 받기\n",
    "    start_date = input('검색 시작 년도(year)를 입력해주세요 : ex)20220505')\n",
    "    end_date = input('검색 끌 일(date)을 입력해주세요 : ex)20220519')\n",
    "    urls = MakeUrl(start_date, end_date)\n",
    "    make_crawling(urls, start_date, end_date)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
